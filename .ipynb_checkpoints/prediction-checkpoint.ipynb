{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbef0f5",
   "metadata": {},
   "source": [
    "Take home assessment: Prediction of bank statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da5f2163-1226-4610-b8eb-df49a8e55865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import transformers\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas\n",
    "import json\n",
    "import matplotlib.pyplot as pyplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1c25f",
   "metadata": {},
   "source": [
    "Importing CSV and Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9369e870-7f98-4890-b892-f5d1c9f2ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSVs and Json config\n",
    "banktranscsv: pandas.DataFrame = pandas.read_csv(\"csv/bank_transaction.csv\")\n",
    "userprofilecsv: pandas.DataFrame = pandas.read_csv(\"csv/user_profile.csv\")\n",
    "descdataseturl: pandas.DataFrame = pandas.read_csv(\"csv/descriptiondf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa154bdf",
   "metadata": {},
   "source": [
    "Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8772ae0d-3a71-4353-afee-587a5a6871a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy both dataset to prevent tempering with original dataset\n",
    "userprofiledf: pandas.DataFrame = userprofilecsv.copy()\n",
    "banktransfiledf: pandas.DataFrame = banktranscsv.copy()\n",
    "descriptiondf: pandas.DataFrame = descdataseturl.copy()\n",
    "\n",
    "# Convert the columns lowercase for consistency\n",
    "userprofiledf.columns = userprofiledf.columns.str.lower()\n",
    "descdataseturl.columns = descdataseturl.columns.str.lower()\n",
    "\n",
    "# Merge both datasets with \"client_id\" being the primary key\n",
    "mergeddf: pandas.DataFrame = pandas.merge(banktransfiledf, userprofiledf, on=\"client_id\", how=\"left\")\n",
    "\n",
    "# filling empty columns with uncategorized \n",
    "mergeddf['category'] = mergeddf['category'].fillna('uncategorized')\n",
    "\n",
    "# Extracting description column for BERT Model training\n",
    "descriptiondf: list = descriptiondf['transaction details'].tolist()\n",
    "descriptiondf: list = list(set(descriptiondf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe49cd4",
   "metadata": {},
   "source": [
    "80:20 split training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf4d536",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns in descriptiondf:\u001b[39m\u001b[38;5;124m\"\u001b[39m, descriptiondf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming 'transaction details' and 'category' are the column names\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Drop duplicates to get unique transactions while preserving category info\u001b[39;00m\n\u001b[1;32m      5\u001b[0m unique_descriptions \u001b[38;5;241m=\u001b[39m descriptiondf\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction details\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "print(\"Columns in descriptiondf:\", descriptiondf.columns.tolist())\n",
    "\n",
    "# Assuming 'transaction details' and 'category' are the column names\n",
    "# Drop duplicates to get unique transactions while preserving category info\n",
    "unique_descriptions = descriptiondf(subset=['transaction details'])\n",
    "\n",
    "# Now do 80/20 split properly\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features (descriptions) and labels (categories)\n",
    "X = unique_descriptions['transaction details'].values\n",
    "y = unique_descriptions['category'].values\n",
    "\n",
    "# Create 80/20 train-test split while maintaining class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,          # 20% for testing\n",
    "    random_state=42,        # For reproducibility\n",
    "    stratify=y              # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Count categories in training and test sets to verify stratification\n",
    "print(\"\\nCategory distribution:\")\n",
    "for dataset_name, dataset in [(\"Training\", y_train), (\"Test\", y_test)]:\n",
    "    unique, counts = np.unique(dataset, return_counts=True)\n",
    "    print(f\"\\n{dataset_name} set:\")\n",
    "    for category, count in sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"  - {category}: {count} ({count/len(dataset)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9653fc-19df-4d81-9432-2790265b7ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Bert Model\n",
    "\n",
    "# model parameter\n",
    "max_length = 128\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "learning_rate = 2e-5\n",
    "\n",
    "def Bertmodel():\n",
    "    # Load pre-trained model\n",
    "    bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Input Layers \n",
    "    inputid = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='inputid')\n",
    "    inputmask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attentionmask')\n",
    "    tokentypeids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids')\n",
    "\n",
    "    # Output\n",
    "    output = bert([inputid, inputmask, tokentypeids])\n",
    "\n",
    "    # Getting CLS token representation\n",
    "    outputsequence = output[0]\n",
    "    cls_token = outputsequence[:, 0, :] # shape: (batch_size, 768)\n",
    "\n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs=[inputid, inputmask, tokentypeids], outputs=cls_token)\n",
    "\n",
    "    return model\n",
    "bert_model = Bertmodel()\n",
    "bert_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fb5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "def XGboost_model(X_train, y_train, X_test, y_test, num_classes):\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        objective='multi:softmax',\n",
    "        num_class=3,\n",
    "        max_depth=6,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        eval_metric='mlogloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "\n",
    "    # Model Traning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e6f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autotune using GridSearchCV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
